# SecondLabAI_1
## Определение фейковых новостей с использованием методов анализа текста

### 1. Постановка задачи, Цель работы. Теоретическая база

#### Постановка задачи

Задача заключается в классификации новостей как "фейк" или "реальная". Необходимо реализовать и обучить нейронную сеть для решения задачи бинарной классификации текстов.

**Требования:**
1.  Собрать или использовать готовый датасет новостей, размеченный по категориям «фейк» / «реальная новость» (например, FakeNewsNet, LIAR, Kaggle Fake News Dataset).
2.  Выполнить предварительную обработку текстов.
3.  Преобразовать тексты в числовое представление (векторы): TF-IDF, Bag-of-Words, Word2Vec или Sentence Transformers.
4.  Обучить модель классификации.
5.  Оценить качество модели по метрикам: Precision, Recall, F1-score, ROC-AUC.
6.  Проанализировать наиболее значимые признаки (слова или выражения), влияющие на решение модели.

**Датасет:** Собираем датасет самостоятельно. Парсим реальные новости и генерируем фейковые новости с помощью языковой модели. Язык русский.
**Векторы:** LLM (Sentence Transformers)
**Модель:** zero-shot-classification, few-shot-classification

#### Цель работы

Научиться реализовывать алгоритмы глубокого обучения, в частности:

*   Работать с текстовыми датасетами
*   Применять предобученные языковые модели (LLM) для векторизации
*   Сравнивать различные подходы к классификации (zero-shot, обучение с нуля)
*   Оценивать качество моделей с помощью метрик Precision, Recall, F1-score, ROC-AUC
*   Анализировать признаки, влияющие на принятие решения моделью

#### Теоретическая база

**Sentence Transformers (LLM)**
Sentence Transformers — это библиотека, позволяющая использовать предобученные трансформеры (например, BERT, RoBERTa, DistilBERT) для создания фиксированных векторных представлений (эмбеддингов) предложений или абзацев. Эти эмбеддинги отражают семантическое значение текста и могут использоваться для задач классификации, кластеризации, поиска и т.д. В работе используется для векторизации текстов новостей.

**Zero-Shot и Few-Shot Classification**
*   **Zero-Shot Classification:** Модель способна классифицировать текст в категории, которые *не видела* во время своего обучения, на основе описания этих категорий и входного текста. Требует предобученной модели, понимающей смысл текста.
*   **Few-Shot Classification:** Модель обучается (или адаптируется) к новой задаче с *очень малым количеством* примеров. В работе представлен как заготовка.

**Logistic Regression**
Логистическая регрессия — это статистическая модель, используемая для задач бинарной или многоклассовой классификации. Несмотря на свою относительную простоту, она часто показывает хорошие результаты, особенно когда признаки (например, эмбеддинги из Sentence Transformers) хорошо отделяют классы. В работе используется как обучаемая модель поверх эмбеддингов.

**Transfer Learning (Дообучение)**
Transfer Learning — это техника, при которой модель, предобученная на большом датасете, адаптируется для решения новой задачи. В контексте Zero-Shot и Few-Shot классификации, это означает использование *предобученной* модели (например, BERT, RoBERTa), которая уже "знает" структуру языка, для решения новой задачи без дополнительного обучения (Zero-Shot) или с минимальным обучением (Few-Shot). В обучаемой модели LogisticRegression используются эмбеддинги, извлеченные предобученной моделью Sentence Transformers.

**Метрики качества**
*   **Accuracy (Точность):** Доля правильно предсказанных примеров среди всех примеров.
    *   `Accuracy = (TP + TN) / (TP + TN + FP + FN)`
*   **Precision (Точность положительного класса):** Доля правильно предсказанных положительных примеров среди всех *предсказанных* положительных.
    *   `Precision = TP / (TP + FP)`
*   **Recall (Полнота положительного класса):** Доля правильно предсказанных положительных примеров среди всех *реальных* положительных.
    *   `Recall = TP / (TP + FN)`
*   **F1-Score:** Гармоническое среднее Precision и Recall.
    *   `F1 = 2 * (Precision * Recall) / (Precision + Recall)`
*   **ROC-AUC (Area Under the ROC Curve):** Метрика, оценивающая качество бинарного классификатора при различных порогах. Чем ближе к 1.0, тем лучше.

### 2. Описание разработанной системы

#### Алгоритмы и принципы работы

##### 2.1 Подготовка данных

*   **Сбор датасета:**
    *   Реальные новости: Используется библиотека `feedparser` для получения новостей из RSS-ленты (например, BBC News).
    *   Фейковые новости: Реализована генерация с помощью языковой модели `gpt2` через библиотеку `transformers`. Модель получает "подсказку" (промпт), например, "Breaking News: A recent study claims that...", и генерирует продолжение, которое считается фейковой новостью.
*   **Разделение данных:** Датасет состоит из равного количества реальных и фейковых новостей (например, 5 + 5 = 10).
*   **Предварительная обработка текста:** Тексты (заголовки и содержание) очищаются от HTML-тегов.

##### 2.2 Архитектура модели

*   **Векторизация (LLM):** Используется `sentence-transformers` (например, `all-MiniLM-L6-v2`) для преобразования текстов в числовые векторы (эмбеддинги).
*   **Zero-Shot Classifier:** Использует `transformers.pipeline` с предобученной моделью (`cointegrated/rubert-base-cased-nli-threeway`) для классификации без обучения на конкретном датасете.
*   **Обучаемая модель (Logistic Regression):** Использует эмбеддинги, полученные из `sentence-transformers`, как признаки для обучения модели `LogisticRegression` с помощью библиотеки `scikit-learn`.

##### 2.3 Процесс обучения

*   Для обучаемой модели `LogisticRegression`:
    *   **Обучение:** Модель обучается на эмбеддингах текстов и их метках (0 - фейк, 1 - реальная).
    *   **Тестирование:** Модель оценивается на том же датасете (в текущей реализации).
*   Для `Zero-Shot Classifier`:
    *   **Классификация:** Модель применяется к текстам без предварительного обучения на них.

##### 2.4 Оценка качества

*   **Метрики:** Вычисляются `accuracy`, `precision`, `recall`, `f1`, `roc_auc` с помощью `scikit-learn`.
*   **Анализ признаков:** Для обучаемой модели `LogisticRegression` анализируются коэффициенты, чтобы понять, какие *измерения* эмбеддинга наиболее важны для принятия решения.

### 3. Результаты работы и тестирования системы

#### 3.1 Параметры эксперимента

*   **Датасет:** Синтетический (5 реальных, 5 сгенерированных `gpt2`).
*   **Векторизация:** `sentence-transformers` (all-MiniLM-L6-v2).
*   **Zero-Shot модель:** `cointegrated/rubert-base-cased-nli-threeway`.
*   **Обучаемая модель:** `LogisticRegression`.
*   **Метрики:** `accuracy`, `precision`, `recall`, `f1`, `roc_auc`.

#### 3.2 Результаты

*   **Zero-Shot Classifier Metrics:**
    *   `accuracy: 0.5000`
    *   `precision: 0.5000`
    *   `recall: 0.4000`
    *   `f1: 0.4444`
    *   `roc_auc: 0.6400`
*   **Trained Classifier Metrics (на тренировочном датасете):**
    *   `accuracy: 1.0000`
    *   `precision: 1.0000`
    *   `recall: 1.0000`
    *   `f1: 1.0000`
    *   `roc_auc: 1.0000` (Переобучение)

### 4. Выводы по работе

#### 4.1 Достигнутые результаты

*   Реализован пайплайн обработки текста для задачи классификации фейковых новостей.
*   Интегрированы `transformers` и `sentence-transformers` для генерации и векторизации текста.
*   Реализованы `ZeroShotClassifier` и обучаемая `LogisticRegression`.
*   Оценено качество моделей с использованием метрик.

#### 4.2 Наблюдения

*   **Эффективность Zero-Shot:** Zero-shot модель показала умеренные результаты на синтетическом датасете, что более реалистично, чем на заглушках.
*   **Переобучение:** Обучаемая модель показывает идеальные метрики на тренировочном датасете (переобучение).
*   **Генерация фейков:** Реализована *настоящая* генерация с помощью `gpt2`, что лучше, чем заглушки.

#### 4.3 Возможные улучшения

*   **Датасет:** Использовать *настоящий* датасет с размеченными фейками и реальными новостями.
*   **Few-Shot:** Реализовать полноценный few-shot подход.
*   **GPU:** Настроить использование GPU для ускорения вычислений.
*   **Дополнительная аугментация:** Использование более сложных техник аугментации текста.
*   **Обработка дисбаланса классов:** Применение техник для работы с несбалансированными данными.

### Инструкция по запуску

#### Требования

*   Python 3.8+
*   PyTorch 2.0+
*   transformers
*   sentence-transformers
*   scikit-learn
*   feedparser

#### Установка зависимостей

```bash
pip install -r requirements.txt




# SecondLabAI_2

## 1. Постановка задачи, Цель работы. Теоретическая база

### Постановка задачи

**Задача 2: Классификация видео**

Современные видеоданные содержат богатую информацию как о визуальных объектах, так и о динамике их взаимодействия во времени. Задача видеоклассификации заключается в автоматическом определении категории или типа действия, изображённого в видеоролике.

Такие системы применяются в широком спектре задач — от анализа пользовательских видео (например, YouTube, TikTok) до систем видеонаблюдения, медиапоиска и робототехники.

Датасет **HowTo100M** содержит более 100 миллионов видеоклипов с естественными субтитрами и является одним из крупнейших ресурсов для мультимодального обучения (видео + текст). Он подходит для задач классификации действий, событий или тематических категорий.

#### План:

*   **Предобработка данных:**
    *   Извлечение кадров из видео с фиксированной частотой.
    *   Нормализация и изменение размера изображений.
*   **Извлечение признаков:**
    *   Визуальные признаки: CNN, 3D-CNN, Vision Transformer, CLIP.
    *   Временные признаки: LSTM, Temporal CNN, TimeSformer.
*   **Обучение модели классификации:**
    *   Определение архитектуры.
    *   Настройка гиперпараметров и функции потерь (например, Cross-Entropy Loss).
    *   Обучение на тренировочном подмножестве и валидация.
*   **Оценка качества:**
    *   Метрики: Accuracy, Precision, Recall, F1-score, Top-K accuracy.
    *   Анализ ошибок (confusion matrix, примеры неверных классификаций).

#### Конкретные параметры задачи:

*   **Датасет:** HowTo100M | Health
*   **Классификация:** Категория внутри Health
*   **Векторы:** VideoMAE (заготовка)
*   **Модель:** Дообучаемая (заготовка)

### Цель работы

Научиться реализовывать алгоритмы глубокого обучения для анализа видео, в частности:

*   Работать с видеодатасетами.
*   Применять transfer learning (дообучение предобученных моделей).
*   Использовать предобученные визуальные модели (ResNet, CLIP, VideoMAE).
*   Сравнивать различные архитектуры моделей и подходы к извлечению признаков.
*   Оценивать качество моделей с помощью метрик Precision, Recall, F1-score, Accuracy.
*   Анализировать признаки, влияющие на принятие решения моделью.

### Теоретическая база

#### VideoMAE (Masked Autoencoders for Video)

VideoMAE — это метод обучения автокодировщиков для видео, использующий маскирование пространственно-временных патчей. Модель обучается восстанавливать пропущенные патчи на основе оставшихся. Полученные эмбеддинги могут быть использованы для задач классификации видео. В работе реализована заготовка для интеграции, но использована fallback-модель из-за сложностей с установкой.

#### CLIP (Contrastive Language-Image Pre-Training)

CLIP — это модель, обученная на парах "изображение-текст". Она извлекает признаки как из изображений, так и из текста в общий векторный (эмбеддинговый) пространство. Это позволяет сопоставлять изображения и текст, а также использовать визуальные признаки CLIP для классификации. В работе используется для извлечения визуальных признаков.

#### ResNet (Residual Network)

ResNet — это архитектура сверточных нейронных сетей, использующая остаточные соединения для преодоления проблемы затухания градиента при обучении глубоких сетей. В работе используется предобученная модель ResNet50 для извлечения визуальных признаков из кадров видео.

#### LSTM (Long Short-Term Memory)

LSTM — это разновидность рекуррентных нейронных сетей (RNN), способная моделировать долгосрочные зависимости в последовательностях. В работе используется для извлечения временных признаков, обрабатывая последовательность визуальных признаков (например, из ResNet) и агрегируя их в один вектор, представляющий динамику видео.

#### Transfer Learning (Дообучение)

Transfer Learning — это техника, при которой модель, предобученная на большом датасете (например, ImageNet, HowTo100M), адаптируется для решения новой задачи. Преимущества:

*   Требуется меньше данных для обучения.
*   Быстрее достигается хорошее качество.
*   Эффективное использование вычислительных ресурсов.

#### Метрики качества

*   **Accuracy (Точность):** Доля правильно предсказанных примеров среди всех примеров.
    `Accuracy = (TP + TN) / (TP + TN + FP + FN)`
*   **Precision (Точность положительного класса):** Доля правильно предсказанных положительных примеров среди всех предсказанных положительных.
    `Precision = TP / (TP + FP)`
*   **Recall (Полнота положительного класса):** Доля правильно предсказанных положительных примеров среди всех реальных положительных.
    `Recall = TP / (TP + FN)`
*   **F1-Score:** Гармоническое среднее Precision и Recall.
    `F1 = 2 * (Precision * Recall) / (Precision + Recall)`
*   **Top-K accuracy:** Процент случаев, когда правильный класс находится среди K самых вероятных предсказаний модели.

---

## 2. Описание разработанной системы

### Алгоритмы и принципы работы

#### 2.1 Подготовка данных (Задача 2)

**Предобработка видео:**

*   **Извлечение кадров:** Кадры извлекаются из видео с фиксированной частотой (FPS), заданной в конфигурации (`fps=0.5` по умолчанию).
*   **Нормализация и изменение размера:** Кадры приводятся к фиксированному размеру (например, 224x224) и нормализуются с использованием статистик ImageNet.
*   **Разделение данных:** Реализовано разделение на train/val/test наборы с использованием `train_test_split`. Для маленького датасета (3 видео) стратифицированное разделение отключено.

#### 2.2 Извлечение признаков (Задача 2)

**Визуальные признаки:**

*   **ResNet50:** Предобученная модель для извлечения признаков из отдельных кадров. Размерность признаков: 2048.
*   **CLIP:** Предобученная модель для извлечения визуальных признаков из кадров. Размерность признаков: 768.
*   **VideoMAE (Заготовка):** Реализована заготовка для извлечения признаков с помощью VideoMAE, но использована fallback-модель из-за проблем с установкой.

**Временные признаки:**

*   **LSTM:** Используется для агрегации последовательности визуальных признаков (например, из ResNet) в один вектор, отражающий динамику.

#### 2.3 Архитектура модели (Задача 2)

**Объединение признаков:**

*   Визуальные и временные признаки объединяются.
*   Для VideoMAE планировалось использовать только визуальные признаки, так как они уже содержат временную информацию.

**Классификация:**

*   Простая нейронная сеть (MLP) на основе `torch.nn` используется для финальной классификации.

#### 2.4 Процесс обучения (Задача 2)

**Дообучение:**

*   Планировалось дообучать предобученную модель (например, VideoMAE) на задаче классификации подкатегорий "Health".
*   В текущей реализации модель обучается с нуля на синтетических данных (например, одном тестовом видео) из-за отсутствия реального датасета.

#### 2.5 Оценка качества

**Метрики:**

*   Для задачи 2 реализованы функции вычисления accuracy, precision, recall, f1, top-k accuracy с помощью `sklearn.metrics`.
*   Анализ ошибок (confusion matrix, примеры неверных классификаций) реализован.

---

## 3. Результаты работы и тестирования системы

### 3.1 Параметры эксперимента (Задача 2)

*   **Датасет:** CSV-файл `howto100m_health_subset.csv`, отфильтрованный по категории "Health", содержащий 3 видео.
*   **Видео:** 3 видео, скачанные из YouTube (например, `OLO20pAroqs.mp4`, `jQyxjAjhNC4.mp4`, `UIUFVdGgaPM.mp4`).
*   **Векторизация:** Использовались модели `ResNet50` (размерность 2048) и `CLIP` (размерность 768). Заготовка для `VideoMAE` (размерность 768) не была активирована из-за проблем с установкой.
*   **Модель:** `VideoClassificationModel` (MLP) с входными размерностями 2048+256 (ResNet+LSTM) или 768+256 (CLIP+LSTM).
*   **Гиперпараметры:** `batch_size=2`, `num_epochs=3`, `learning_rate=1e-4`.
*   **Метрики:** accuracy, precision, recall, f1, top-k accuracy.

### 3.2 Результаты (Задача 2)

**Извлечение признаков:**

*   Успешно реализованы компоненты для ResNet, CLIP, LSTM.
*   VideoMAE: Заготовка реализована, fallback-модель работает.

**Обучение и оценка:**

*   При использовании 3 видео из разных подкатегорий (`Conditions and Treatments`, `Injury and Accidents`, `Medication and Medical Equipment`) и модели ResNet+LSTM, результаты показывают, что модель не смогла научиться различать классы на тестовом наборе (accuracy=0.0000), что объясняется крайне малым количеством данных (по одному видео на класс).
*   При использовании одного видео (все 3 видео из одной подкатегории) модель показала идеальные результаты (accuracy=1.0000), что является следствием переобучения на маленьком датасете.
